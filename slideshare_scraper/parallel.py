#!/usr/bin/env python3
"""
SlideShare ä¸¦è¡Œè™•ç†æ¨¡çµ„

åŒ…å«ä¸¦è¡Œçˆ¬å–ã€ä»»å‹™åŸ·è¡Œã€é‡è©¦æ©Ÿåˆ¶ç­‰åŠŸèƒ½ã€‚
"""

import os
import time
import random
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict

from .scraper import SlideShareScraper
from .utils import get_category_url, count_csv_data, find_latest_files_by_pattern
from .constants import DEFAULT_SETTINGS


def warmup_webdriver():
    """
    é ç†± WebDriverï¼Œç¢ºä¿é©…å‹•ç¨‹å¼å·²ä¸‹è¼‰
    
    Returns:
        æ˜¯å¦æˆåŠŸé ç†±
    """
    try:
        print("ğŸ”§ æ­£åœ¨é ç†± WebDriver...")
        scraper = SlideShareScraper(download_num=1, headless=True, section_type="featured")
        scraper.setup_driver()
        if scraper.driver:
            scraper.driver.quit()
        print("âœ… WebDriver é ç†±å®Œæˆ")
        return True
    except Exception as e:
        print(f"âš ï¸  WebDriver é ç†±å¤±æ•—ï¼š{e}")
        return False


def execute_single_task(task_info: Dict) -> Dict:
    """
    åŸ·è¡Œå–®ä¸€çˆ¬å–ä»»å‹™ï¼ˆå¸¶é‡è©¦æ©Ÿåˆ¶ï¼‰

    Args:
        task_info: åŒ…å«ä»»å‹™è³‡è¨Šçš„å­—å…¸

    Returns:
        ä»»å‹™åŸ·è¡Œçµæœ
    """
    category = task_info["category"]
    section = task_info["section"]
    download_num = task_info["download_num"]
    headless = task_info["headless"]
    task_id = task_info["task_id"]
    max_retries = task_info.get("max_retries", DEFAULT_SETTINGS["max_retries"])

    result = {
        "task_id": task_id,
        "category": category,
        "section": section,
        "success": False,
        "error": None,
        "data_count": 0,
        "filename": None,
        "retry_count": 0
    }

    for attempt in range(max_retries + 1):
        try:
            if attempt > 0:
                # é‡è©¦å‰ç­‰å¾…éš¨æ©Ÿæ™‚é–“ï¼Œé¿å…åŒæ™‚é‡è©¦
                wait_time = random.uniform(*DEFAULT_SETTINGS["retry_delay_range"])
                print(f"[ä»»å‹™ {task_id}] ç¬¬ {attempt + 1} æ¬¡å˜—è©¦ï¼Œç­‰å¾… {wait_time:.1f} ç§’...")
                time.sleep(wait_time)

            print(f"[ä»»å‹™ {task_id}] é–‹å§‹çˆ¬å– {category} é¡åˆ¥çš„ {section} å€å¡Š")

            url = get_category_url(category)

            # æ·»åŠ å•Ÿå‹•å»¶é²ï¼Œé¿å…åŒæ™‚å•Ÿå‹•å¤ªå¤šç€è¦½å™¨
            startup_delay = random.uniform(*DEFAULT_SETTINGS["startup_delay_range"])
            time.sleep(startup_delay)

            scraper = SlideShareScraper(
                download_num=download_num,
                headless=headless,
                section_type=section
            )

            # åŸ·è¡Œçˆ¬å–
            scraper.scrape_slideshare(url)

            # æª¢æŸ¥æ˜¯å¦æˆåŠŸç”Ÿæˆæª”æ¡ˆ
            section_name = section.capitalize()
            output_dir = "output_url"

            # æœå°‹æ‰€æœ‰å¯èƒ½çš„æª”æ¡ˆåç¨±æ ¼å¼
            possible_files = find_latest_files_by_pattern(output_dir, f"_{section_name}.csv")

            # æ‰¾åˆ°æœ€æ–°çš„æª”æ¡ˆï¼ˆå‡è¨­æ˜¯å‰›å‰›ç”Ÿæˆçš„ï¼‰
            if possible_files:
                latest_file = possible_files[0]  # å·²ç¶“æŒ‰æ™‚é–“æ’åº
                filepath = os.path.join(output_dir, latest_file)

                # è¨ˆç®—è³‡æ–™ç­†æ•¸
                data_count = count_csv_data(filepath)

                result.update({
                    "success": True,
                    "data_count": data_count,
                    "filename": latest_file,
                    "retry_count": attempt
                })
                print(f"[ä»»å‹™ {task_id}] âœ… å®Œæˆï¼çˆ¬å– {data_count} ç­†è³‡æ–™ â†’ {latest_file}")
                return result

            raise FileNotFoundError("æª”æ¡ˆæœªç”Ÿæˆ")

        except Exception as e:
            result["error"] = str(e)
            result["retry_count"] = attempt

            if attempt < max_retries:
                print(f"[ä»»å‹™ {task_id}] âš ï¸  ç¬¬ {attempt + 1} æ¬¡å˜—è©¦å¤±æ•—ï¼š{e}ï¼Œæº–å‚™é‡è©¦...")
            else:
                print(f"[ä»»å‹™ {task_id}] âŒ æœ€çµ‚å¤±æ•—ï¼š{e}")

    return result


def execute_parallel_tasks(tasks: List[Dict], window_num: int) -> List[Dict]:
    """
    ä¸¦è¡ŒåŸ·è¡Œå¤šå€‹çˆ¬å–ä»»å‹™ï¼ˆå„ªåŒ–ç‰ˆï¼‰

    Args:
        tasks: ä»»å‹™åˆ—è¡¨
        window_num: ä¸¦è¡Œè¦–çª—æ•¸é‡

    Returns:
        ä»»å‹™åŸ·è¡Œçµæœåˆ—è¡¨
    """
    print(f"é–‹å§‹ä¸¦è¡ŒåŸ·è¡Œ {len(tasks)} å€‹ä»»å‹™ï¼Œä½¿ç”¨ {window_num} å€‹ä¸¦è¡Œè¦–çª—")
    print("=" * 70)

    # ç‚ºæ¯å€‹ä»»å‹™æ·»åŠ é‡è©¦è¨­å®š
    for task in tasks:
        task["max_retries"] = DEFAULT_SETTINGS["max_retries"]

    results = []
    start_time = time.time()

    with ThreadPoolExecutor(max_workers=window_num) as executor:
        # æäº¤æ‰€æœ‰ä»»å‹™
        future_to_task = {
            executor.submit(execute_single_task, task): task
            for task in tasks
        }

        # æ”¶é›†çµæœ
        for future in as_completed(future_to_task):
            try:
                result = future.result()
                results.append(result)

                # é¡¯ç¤ºé€²åº¦
                completed = len(results)
                total = len(tasks)
                success_count = sum(1 for r in results if r["success"])
                elapsed_time = time.time() - start_time

                print(f"é€²åº¦ï¼š{completed}/{total} å€‹ä»»å‹™å®Œæˆ | æˆåŠŸï¼š{success_count} | è€—æ™‚ï¼š{elapsed_time:.1f}s")

            except Exception as e:
                # è™•ç†åŸ·è¡Œç·’ç•°å¸¸
                task = future_to_task[future]
                error_result = {
                    "task_id": task["task_id"],
                    "category": task["category"],
                    "section": task["section"],
                    "success": False,
                    "error": f"åŸ·è¡Œç·’ç•°å¸¸ï¼š{e}",
                    "data_count": 0,
                    "filename": None,
                    "retry_count": 0
                }
                results.append(error_result)
                print(f"[ä»»å‹™ {task['task_id']}] âŒ åŸ·è¡Œç·’ç•°å¸¸ï¼š{e}")

    return results


def retry_failed_tasks(failed_tasks: List[Dict], args) -> None:
    """
    é‡è©¦å¤±æ•—çš„ä»»å‹™

    Args:
        failed_tasks: å¤±æ•—ä»»å‹™åˆ—è¡¨
        args: å‘½ä»¤åˆ—åƒæ•¸
    """
    print(f"\nğŸ”„ é–‹å§‹é‡è©¦ {len(failed_tasks)} å€‹å¤±æ•—ä»»å‹™...")
    print("ä½¿ç”¨è¼ƒä¿å®ˆçš„è¨­å®šï¼š2 å€‹ä¸¦è¡Œè¦–çª—ï¼Œå¢åŠ å»¶é²æ™‚é–“")

    # æº–å‚™é‡è©¦ä»»å‹™åˆ—è¡¨
    retry_tasks = []
    for i, failed_task in enumerate(failed_tasks, 1):
        retry_tasks.append({
            "task_id": f"é‡è©¦-{i}",
            "category": failed_task["category"],
            "section": failed_task["section"],
            "download_num": args.num,
            "headless": args.headless,
            "max_retries": 3  # å¢åŠ é‡è©¦æ¬¡æ•¸
        })

    # ä½¿ç”¨è¼ƒå°‘çš„ä¸¦è¡Œè¦–çª—é‡è©¦
    retry_window_num = min(2, len(retry_tasks))
    retry_results = execute_parallel_tasks(retry_tasks, retry_window_num)

    # é¡¯ç¤ºé‡è©¦çµæœ
    retry_successful = [r for r in retry_results if r["success"]]
    retry_failed = [r for r in retry_results if not r["success"]]

    print("\nğŸ”„ é‡è©¦çµæœæ‘˜è¦ï¼š")
    print(f"âœ… é‡è©¦æˆåŠŸï¼š{len(retry_successful)}/{len(failed_tasks)}")
    print(f"âŒ ä»ç„¶å¤±æ•—ï¼š{len(retry_failed)}/{len(failed_tasks)}")

    if retry_successful:
        print("\nğŸ“ é‡è©¦æˆåŠŸçš„æª”æ¡ˆï¼š")
        for result in retry_successful:
            print(f"   - {result['filename']} ({result['data_count']} ç­†)")

    if retry_failed:
        print("\nâš ï¸  ä»ç„¶å¤±æ•—çš„ä»»å‹™ï¼š")
        for result in retry_failed:
            print(f"   - {result['category']}_{result['section']}: {result['error']}")
